{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e03977",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN) - Lab\n",
    "#### Author: Kamil Barszczak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a09c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers, models, utils, regularizers, optimizers, losses\n",
    "from tensorflow import keras, data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b11ac",
   "metadata": {},
   "source": [
    "#### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4442768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'E:/Data/CUB_200_2011'\n",
    "train_test_split = 0.9\n",
    "width, height = 64, 64\n",
    "latent_dim = 64\n",
    "batch = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf54fff",
   "metadata": {},
   "source": [
    "#### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c9b483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11788it [00:53, 220.46it/s]\n"
     ]
    }
   ],
   "source": [
    "bboxes = pd.read_csv(os.path.join(dataset_path, 'bounding_boxes.txt'), sep = \" \", names=[\"id\", \"x\", \"y\", \"width\", \"height\"]).astype(int)\n",
    "annotations = pd.read_csv(os.path.join(dataset_path, 'images.txt'), sep = \" \", names=[\"id\", \"path\"])\n",
    "\n",
    "processed = []\n",
    "for index, row in tqdm(annotations.iterrows()):\n",
    "    image_id = row.id\n",
    "    image_path = row.path\n",
    "    bbox  = bboxes[bboxes.id == image_id].iloc[0]\n",
    "    \n",
    "    img = cv2.imread(os.path.join(dataset_path, \"images\", image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img[bbox.y:bbox.y + bbox.height, bbox.x:bbox.x + bbox.width, :]\n",
    "    img = cv2.resize(img, (width, height))\n",
    "    img = tf.cast(img, 'float32') / 127.5 - 1\n",
    "    processed.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd7674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: <BatchDataset element_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None)>\n",
      "Test dataset: <BatchDataset element_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "processed = np.array(processed)\n",
    "np.random.shuffle(processed)\n",
    "split = int(len(processed) * train_test_split)\n",
    "\n",
    "train_dataset = data.Dataset.from_tensor_slices(processed[:split]) \\\n",
    "                .shuffle(1024, reshuffle_each_iteration=True) \\\n",
    "                .batch(batch)\n",
    "\n",
    "test_dataset = data.Dataset.from_tensor_slices(processed[split:]) \\\n",
    "                .batch(batch)\n",
    "\n",
    "print(\"Train dataset:\", train_dataset)\n",
    "print(\"Test dataset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d409b1b",
   "metadata": {},
   "source": [
    "#### Create custom layers and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0174e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProgress(keras.callbacks.Callback):\n",
    "    def __init__(self, latent_dim, images=6, every_epoch=10):\n",
    "        self.random_vectors = tf.random.normal((images, latent_dim))\n",
    "        self.every_epoch = every_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch) % self.every_epoch == 0:\n",
    "            generated_images = self.model.generator(self.random_vectors, training=False)\n",
    "        \n",
    "            fig = plt.figure(figsize=(12, 8))\n",
    "            for i, image in enumerate(generated_images):\n",
    "                fig.add_subplot(1, len(self.random_vectors), i + 1)\n",
    "                plt.imshow((image.numpy() + 1) * 0.5)\n",
    "                \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328514a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleBlock(layers.Layer):\n",
    "    def __init__(self, filters, size, strides=2, activation=layers.LeakyReLU(0.2), use_normalization=True, dropout=0.0, **kwargs):\n",
    "        super(DownsampleBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.size = size\n",
    "        self.strides = strides\n",
    "        self.activation = activation\n",
    "        self.use_normalization = use_normalization\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.stack = keras.Sequential()\n",
    "        self.stack.add(layers.Conv2D(filters, size, strides, 'same'))\n",
    "        \n",
    "        if use_normalization:\n",
    "            self.stack.add(layers.BatchNormalization(momentum=0.5))\n",
    "            \n",
    "        self.stack.add(activation)\n",
    "        \n",
    "        if dropout > 0:\n",
    "            self.stack.add(layers.Dropout(dropout))\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"size\": self.size,\n",
    "            \"strides\": self.strides,\n",
    "            \"activation\": self.activation,\n",
    "            \"use_normalization\": self.use_normalization,\n",
    "            \"dropout\": self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1dfe84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBlock(layers.Layer):\n",
    "    def __init__(self, filters, size, strides=2, activation=layers.ReLU(), use_normalization=True, dropout=0.0, **kwargs):\n",
    "        super(UpsampleBlock, self).__init__(**kwargs)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.size = size\n",
    "        self.strides = strides\n",
    "        self.activation = activation\n",
    "        self.use_normalization = use_normalization\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.stack = keras.Sequential()\n",
    "        self.stack.add(layers.Conv2DTranspose(filters, size, strides, 'same'))\n",
    "        \n",
    "        if use_normalization:\n",
    "            self.stack.add(layers.BatchNormalization(momentum=0.5))\n",
    "            \n",
    "        self.stack.add(activation)\n",
    "        \n",
    "        if dropout > 0:\n",
    "            self.stack.add(layers.Dropout(dropout))\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"size\": self.size,\n",
    "            \"strides\": self.strides,\n",
    "            \"activation\": self.activation,\n",
    "            \"use_normalization\": use_self.normalization,\n",
    "            \"dropout\": self.dropout\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a5aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(layers.Layer):\n",
    "    def __init__(self, output_activation='tanh', output_strides=1, output_kernel_size=3,\n",
    "                 dense_layers_info=[\n",
    "                     ((4, 4, 256), True, layers.ReLU(), 0)\n",
    "                 ],  # shape, normalization, activation, dropout,\n",
    "                 upsample_layers_info=[\n",
    "                     (256, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                     (128, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                     (64, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                     (32, 3, 2, True, layers.LeakyReLU(0.2), 0.0)\n",
    "                 ]  # filters, size, strides, normalization, activation, dropout\n",
    "                 , **kwargs):\n",
    "        super(Generator, self).__init__(**kwargs)\n",
    "        \n",
    "        self.output_activation = output_activation\n",
    "        self.output_strides = output_strides\n",
    "        self.output_kernel_size = output_kernel_size\n",
    "        self.dense_layers_info = dense_layers_info\n",
    "        self.upsample_layers_info = upsample_layers_info\n",
    "            \n",
    "        self.stack = keras.Sequential()\n",
    "        \n",
    "        for index, dl in enumerate(dense_layers_info):\n",
    "            self.stack.add(layers.Dense(dl[0][0]*dl[0][1]*dl[0][2], name=f\"gen_dense_{index}\"))\n",
    "            if dl[1]:\n",
    "                self.stack.add(layers.BatchNormalization(momentum=0.5))\n",
    "            if dl[2] is not None:\n",
    "                self.stack.add(dl[2])\n",
    "            if dl[3] > 0:\n",
    "                self.stack.add(layers.Dropout(dl[3]))\n",
    "            \n",
    "        self.stack.add(layers.Reshape(dense_layers_info[-1][0], name=\"gen_reshape\"))\n",
    "        \n",
    "        for index, ul in enumerate(upsample_layers_info):\n",
    "            self.stack.add(UpsampleBlock(\n",
    "                filters=ul[0],\n",
    "                size=ul[1],\n",
    "                strides=ul[2],\n",
    "                use_normalization=ul[3], \n",
    "                activation=ul[4], \n",
    "                dropout=ul[5], \n",
    "                name=f\"gen_up_{index}\"\n",
    "            ))\n",
    "        \n",
    "        self.stack.add(layers.Conv2D(3, kernel_size=output_kernel_size, strides=output_strides, activation=output_activation, padding=\"same\", name=\"gen_conv_outputs\"))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_activation\": self.output_activation,\n",
    "            \"output_strides\": self.output_strides,\n",
    "            \"output_kernel_size\": self.output_kernel_size,\n",
    "            \"dense_layers_info\": self.dense_layers_info,\n",
    "            \"upsample_layers_info\": self.upsample_layers_info,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902e4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 output_activation='sigmoid',\n",
    "                 dense_layers_info=[\n",
    "                     (128, True, layers.ReLU(), 0.0)\n",
    "                 ],  # shape, normalization, activation, dropout\n",
    "                 downsample_layers_info=[\n",
    "                     (64, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                     (128, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                     (256, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "                 ]  # filters, size, strides, normalization, activation, dropout\n",
    "                 , **kwargs):\n",
    "        super(Discriminator, self).__init__(**kwargs)\n",
    "            \n",
    "        self.output_activation = output_activation\n",
    "        self.dense_layers_info = dense_layers_info\n",
    "        self.downsample_layers_info = downsample_layers_info\n",
    "        \n",
    "        self.stack = keras.Sequential()\n",
    "        \n",
    "        for index, ul in enumerate(downsample_layers_info):\n",
    "            self.stack.add(DownsampleBlock(\n",
    "                filters=ul[0],\n",
    "                size=ul[1],\n",
    "                strides=ul[2],\n",
    "                use_normalization=ul[3], \n",
    "                activation=ul[4], \n",
    "                dropout=ul[5], \n",
    "                name=f\"dis_down_{index}\"\n",
    "            ))\n",
    "            \n",
    "        self.stack.add(layers.Flatten(name=\"dis_flatten\"))\n",
    "            \n",
    "        for index, dl in enumerate(dense_layers_info):\n",
    "            self.stack.add(layers.Dense(dl[0], name=f\"dis_dense_{index}\"))\n",
    "            if dl[1]:\n",
    "                self.stack.add(layers.BatchNormalization(momentum=0.5))\n",
    "            if dl[2] is not None:\n",
    "                self.stack.add(dl[2])\n",
    "            if dl[3] > 0:\n",
    "                self.stack.add(layers.Dropout(dl[3]))\n",
    "        \n",
    "        self.stack.add(layers.Dense(1, activation=output_activation, name=\"gen_conv_outputs\"))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_activation\": self.output_activation,\n",
    "            \"dense_layers_info\": self.dense_layers_info,\n",
    "            \"downsample_layers_info\": self.downsample_layers_info,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.stack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32093c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGAN\u001b[39;00m(\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mModel):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, generator, discriminator):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def compile(self, g_optimizer, d_optimizer, g_loss_fn, d_loss_fn, latent_dim, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        \n",
    "        self.g_loss_mean = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.d_loss_mean = keras.metrics.Mean(name=\"d_loss\")\n",
    "        \n",
    "        self.real_acc = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_acc = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        \n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        \n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.g_loss_mean,\n",
    "            self.d_loss_mean,\n",
    "            self.real_acc,\n",
    "            self.generated_acc,\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # 1. Train the discriminator\n",
    "        \n",
    "        # Generate some images\n",
    "        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim)) # Get noise for generator input\n",
    "        generated_images = self.generator(random_noise) # Generate images for discriminator training step\n",
    "        \n",
    "        # Create labels for the discriminator\n",
    "        # We assume 1 - real image, 0 - fake image + noise (important trick!)\n",
    "        real_labels = tf.ones((batch_size, 1)) -  tf.random.uniform((batch_size,1)) * 0.15\n",
    "        generated_labels = tf.random.uniform((batch_size,1)) * 0.15\n",
    "        \n",
    "        # Train the network, update weights manually\n",
    "        # GradientTape is used to record all calculations inside the network and calculate gradient during backprop\n",
    "        with tf.GradientTape() as tape:            \n",
    "            # Divide into two steps because of BatchNormalization\n",
    "            real_preds = self.discriminator(real_images)            # Predict classes for real images\n",
    "            generated_preds = self.discriminator(generated_images)  # Predict classes for generated images\n",
    "\n",
    "            # Labels and predictions of [real,fake] are merged for a discriminator loss function\n",
    "            d_loss = self.d_loss_fn(\n",
    "                tf.concat([real_labels, generated_labels], axis=0), # true labels\n",
    "                tf.concat([real_preds, generated_preds], axis=0)  # predicted labels\n",
    "            )     \n",
    "        \n",
    "        d_loss = tf.cond(\n",
    "            tf.math.is_finite(d_loss),\n",
    "            true_fn=lambda: d_loss,\n",
    "            false_fn=lambda: 0\n",
    "        )\n",
    "          \n",
    "        # Calculate discriminator gradients\n",
    "        d_grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        # Apply weight changes\n",
    "        self.d_optimizer.apply_gradients(zip(d_grads,self.discriminator.trainable_weights))\n",
    "\n",
    "        #2. Train the generator\n",
    "        #[2*] Because discriminator is trained on 2*batch_size number of samples (fake and real)\n",
    "        random_noise = tf.random.normal(shape=(2*batch_size, self.latent_dim)) #get noise for generator input\n",
    "        \n",
    "        # During training, the generator tries to deceive the discriminator that it can generate realistic images\n",
    "        # Therefore, we use \"real (1)\" labels for the generator\n",
    "        misleading_labesl = tf.ones((2*batch_size, 1))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator(random_noise) # Generate images\n",
    "            fake_preds = self.discriminator(generated_images,training=False)\n",
    "            g_loss = self.g_loss_fn(misleading_labesl, fake_preds)  \n",
    "      \n",
    "        \n",
    "        g_loss = tf.cond(\n",
    "            tf.math.is_finite(g_loss),\n",
    "            true_fn=lambda: g_loss,\n",
    "            false_fn=lambda: 0\n",
    "        )\n",
    "        \n",
    "        # Calculate generator gradients\n",
    "        g_grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "\n",
    "        # Apply weight changes\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads,self.generator.trainable_weights))\n",
    "        \n",
    "        # Uupdate metrics\n",
    "        self.g_loss_mean.update_state(g_loss)\n",
    "        self.d_loss_mean.update_state(d_loss)\n",
    "        self.real_acc.update_state(1.0, real_preds)\n",
    "        self.generated_acc.update_state(0.0, generated_preds)\n",
    "            \n",
    "        return {'g_loss': g_loss, 'd_loss': d_loss, \n",
    "               'real_acc': self.real_acc.result(), 'gen_acc': self.generated_acc.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88652e49",
   "metadata": {},
   "source": [
    "#### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e36186bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " generator (Generator)       (None, 64, 64, 3)         6078851   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,078,851\n",
      "Trainable params: 6,076,931\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(\n",
    "    dense_layers_info=[\n",
    "        ((4, 4, 128), False, layers.ReLU(), 0.1)\n",
    "    ],\n",
    "    upsample_layers_info=[\n",
    "        (512, 5, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (256, 5, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (128, 5, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (64, 5, 2, True, layers.LeakyReLU(0.2), 0.0)\n",
    "    ],\n",
    "    name=\"generator\"\n",
    ")\n",
    "\n",
    "gen_inputs = layers.Input(shape=(latent_dim, ))\n",
    "gen_outputs = gen(gen_inputs)\n",
    "generator = keras.Model(inputs=gen_inputs, outputs=gen_outputs)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a5c01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " discriminator (Discriminato  (None, 1)                914881    \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 914,881\n",
      "Trainable params: 913,921\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dis = Discriminator(\n",
    "    output_activation='sigmoid',\n",
    "    dense_layers_info=[\n",
    "        (128, False, layers.ReLU(), 0.1)\n",
    "    ],\n",
    "    downsample_layers_info=[\n",
    "        (32, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (64, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (128, 3, 2, True, layers.LeakyReLU(0.2), 0.0),\n",
    "        (256, 3, 2, True, layers.LeakyReLU(0.2), 0.0)\n",
    "    ],\n",
    "    name=\"discriminator\"\n",
    ")\n",
    "dis_inputs = layers.Input(shape=(width, height, 3))\n",
    "dis_outputs = dis(dis_inputs)\n",
    "discriminator = keras.Model(inputs=dis_inputs, outputs=dis_outputs)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43ed061",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(\n",
    "    generator,\n",
    "    discriminator\n",
    ")\n",
    "\n",
    "gan.compile(\n",
    "    optimizers.Adam(0.00015, beta_1=0.5, beta_2=0.9),  # generator\n",
    "    optimizers.Adam(0.0002, beta_1=0.5, beta_2=0.9),  # discriminator\n",
    "    losses.BinaryCrossentropy(),\n",
    "    losses.BinaryCrossentropy(),\n",
    "    latent_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652b088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Kamil\\AppData\\Local\\Temp\\ipykernel_2340\\4245396914.py\", line 60, in train_step\n        if tf.math.is_finite(d_loss):\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mImageProgress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1233\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1234\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Kamil\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Kamil\\AppData\\Local\\Temp\\ipykernel_2340\\4245396914.py\", line 60, in train_step\n        if tf.math.is_finite(d_loss):\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "history = gan.fit(\n",
    "    train_dataset,\n",
    "    epochs=250,\n",
    "    callbacks=[ImageProgress(latent_dim)]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70961",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights(\"generator.h5\")\n",
    "discriminator.save_weights(\"discriminator.h5\")\n",
    "\n",
    "# https://stackoverflow.com/questions/69498990/operatornotallowedingrapherror-using-a-tf-tensor-as-a-python-bool-is-not-al"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
